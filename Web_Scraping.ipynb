{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-08T20:24:33.502501Z",
     "start_time": "2024-12-08T20:24:33.497309Z"
    }
   },
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import json"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:24:10.379709Z",
     "start_time": "2024-12-08T20:24:09.876759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "1. Extract Text Data:\n",
    "\"\"\"\n",
    "url = \"https://www.baraasallout.com/test.html\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "data = []\n",
    "\n",
    "for h1 in soup.find_all('h1'):\n",
    "    data.append([\"Heading\", h1.get_text(strip=True)])\n",
    "\n",
    "for h2 in soup.find_all('h2'):\n",
    "    data.append([\"Heading\", h2.get_text(strip=True)])\n",
    "\n",
    "for p in soup.find_all('p'):\n",
    "    data.append([\"Paragraph\", p.get_text(strip=True)])\n",
    "\n",
    "for li in soup.find_all('li'):\n",
    "    data.append([\"List Item\", li.get_text(strip=True)])\n",
    "\n",
    "# Write data to a CSV file\n",
    "with open('Extract_Text_Data.CSV', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Type\", \"Content\"])\n",
    "    writer.writerows(data)\n",
    "    f.close()\n",
    "\n",
    "print(\"Data extraction and CSV creation completed successfully!\")"
   ],
   "id": "19bb2b950b02c661",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and CSV creation completed successfully!\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:14:24.775359Z",
     "start_time": "2024-12-08T20:14:24.758351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "2. Extract Table Data:\n",
    "\"\"\"\n",
    "table = soup.find('table')\n",
    "table_head = []\n",
    "for th in soup.find_all('th'):\n",
    "    table_head.append(th.get_text(strip=True))\n",
    "table_data = []\n",
    "for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "    cells = [cell.get_text(strip=True) for cell in row.find_all('td')]\n",
    "    table_data.append(cells)\n",
    "with open('Extract_Table_Data.CSV', 'w', encoding='UTF8', newline='') as f :\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(table_head)\n",
    "    writer.writerows(table_data)\n",
    "    f.close()\n",
    "\n",
    "print(\"Data extraction and CSV creation completed successfully!\")"
   ],
   "id": "70587ce082667244",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and CSV creation completed successfully!\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:59:45.708247Z",
     "start_time": "2024-12-08T20:59:45.698196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "3. Extract Product Information (Cards Section):\n",
    "'''\n",
    "products = []\n",
    "\n",
    "product_cards = soup.find_all('div', class_='product-card')\n",
    "\n",
    "for card in product_cards:\n",
    "    product_id = card.get('data-id', 'N/A')\n",
    "    name = card.find('p', class_='name').get_text(strip=True) if card.find('p', class_='name') else 'N/A'\n",
    "    price = card.find('p', class_='price')\n",
    "    hidden_price = price.get_text(strip=True) if price else 'N/A'\n",
    "    colors = card.find('p', class_='colors').get_text(strip=True) if card.find('p', class_='colors') else 'N/A'\n",
    "\n",
    "    product_info = {\n",
    "        'id': product_id,\n",
    "        'name': name,\n",
    "        'price': hidden_price,\n",
    "        'colors': colors\n",
    "    }\n",
    "\n",
    "    products.append(product_info)\n",
    "\n",
    "with open('Featured_Products.JSON', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(products, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Featured product information has been extracted and saved to Featured_Products.JSON.\")\n",
    "\n"
   ],
   "id": "3ec0f7bd9df69a61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featured product information has been extracted and saved to Featured_Products.JSON.\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:50:35.758959Z",
     "start_time": "2024-12-08T20:50:35.735302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "4. Extract Form Details:\n",
    "'''\n",
    "\n",
    "form = soup.find('form')\n",
    "\n",
    "form_details = []\n",
    "for input_field in form.find_all('input'):\n",
    "    field_name = input_field.get('name', 'N/A')\n",
    "    input_type = input_field.get('type', 'text')\n",
    "    default_value = input_field.get('value', '')\n",
    "\n",
    "    form_details.append({\n",
    "        \"Field Name\": field_name,\n",
    "        \"Input Type\": input_type,\n",
    "        \"Default Value\": default_value\n",
    "    })\n",
    "\n",
    "with open('Form_Details.JSON', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(form_details, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Form details have been extracted and saved to Form_Details.JSON.\")\n"
   ],
   "id": "fecb88c556778989",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Form details have been extracted and saved to Form_Details.JSON.\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:53:27.733862Z",
     "start_time": "2024-12-08T20:53:27.724338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "5. Extract Links and Multimedia:\n",
    "'''\n",
    "\n",
    "links_and_media = []\n",
    "\n",
    "for anchor in soup.find_all('a', href=True):\n",
    "    link_info = {\n",
    "        \"Type\": \"Hyperlink\",\n",
    "        \"Text\": anchor.get_text(strip=True),\n",
    "        \"URL\": anchor['href']\n",
    "    }\n",
    "    links_and_media.append(link_info)\n",
    "\n",
    "for iframe in soup.find_all('iframe', src=True):\n",
    "    video_info = {\n",
    "        \"Type\": \"Video\",\n",
    "        \"Source URL\": iframe['src']\n",
    "    }\n",
    "    links_and_media.append(video_info)\n",
    "\n",
    "with open('Links_and_Multimedia.JSON', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(links_and_media, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Links and multimedia data have been extracted and saved to Links_and_Multimedia.JSON.\")"
   ],
   "id": "b88c346d3f4d8f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links and multimedia data have been extracted and saved to Links_and_Multimedia.JSON.\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "61436cf374975c1f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
